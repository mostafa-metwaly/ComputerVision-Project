
@software{glenn_jocher_ultralyticsyolov5_2021,
	title = {ultralytics/yolov5: v4.0 - nn.{SiLU}() activations, Weights \& Biases logging, {PyTorch} Hub integration},
	url = {https://zenodo.org/record/4418161#.YDfMQugzZPY},
	shorttitle = {ultralytics/yolov5},
	abstract = {This release implements two architecture changes to {YOLOv}5, as well as various bug fixes and performance improvements. Breaking Changes nn.{SiLU}() activations replace nn.{LeakyReLU}(0.1) and nn.Hardswish() activations used in previous versions. nn.{SiLU}() was introduced in {PyTorch} 1.7.0 (https://pytorch.org/docs/stable/generated/torch.nn.{SiLU}.html), and due to the recent timeframe certain export pipelines may be temporarily unavailable ({CoreML} possibly) without updates to the associated tools (i.e. coremltools). Bug Fixes Multi-{GPU} --resume \#1810 leaf Variable inplace bug fix \#1759 Various bug fixes contained in {PRs} \#1235 through \#1837 Added Functionality Weights \& Biases (W\&B) Feature Addition \#1235 Utils reorganization \#1392 {PyTorch} Hub and {autoShape} update \#1415 W\&B artifacts feature addition \#1712 Various additional feature additions contained in {PRs} \#1235 through \#1837 Updated Results Latest models are all slightly smaller to due removal of one convolution within each bottleneck, which have been renamed as C3() modules now in light of the 3 I/O convolutions each one does vs the 4 in the standard {CSP} bottleneck. The previous manual concatenation and {LeakyReLU}(0.1) activations have both removed, simplifying the architecture, reducing parameter count, and better exploiting the .fuse() operation at inference time. nn.{SiLU}() activations replace nn.{LeakyReLU}(0.1) and nn.Hardswish() activations throughout the model, simplifying the architecture as we now only have one single activation function used everywhere rather than the two types before. In general the changes result in smaller models (89.0M params -{\textgreater} 87.7M {YOLOv}5x), faster inference times (6.9ms -{\textgreater} 6.0ms), and improved {mAP} (49.2 -{\textgreater} 50.1) for all models except {YOLOv}5s, which reduced {mAP} slightly (37.0 -{\textgreater} 36.8). In general the largest models benefit the most from this update. {YOLOv}5x in particular is now above 50.0 {mAP} at --img-size 640, which may be the first time this is possible at 640 resolution for any architecture I'm aware of (correct me if I'm wrong though). {\textless}img src="https://user-images.githubusercontent.com/26833433/103594689-455e0e00-4eae-11eb-9cdf-7d753e2ceeeb.png" width="1000"{\textgreater}** {GPU} Speed measures end-to-end time per image averaged over 5000 {COCO} val2017 images using a V100 {GPU} with batch size 32, and includes image preprocessing, {PyTorch} {FP}16 inference, postprocessing and {NMS}. {EfficientDet} data from google/automl at batch size 8. Pretrained Checkpoints Model size {AP}$^{\textrm{val}}$ {AP}$^{\textrm{test}}$ {AP}$_{\textrm{50}}$ Speed$_{\textrm{V100}}$ {FPS}$_{\textrm{V100}}$ params {GFLOPS} {YOLOv}5s 640 36.8 36.8 55.6 2.2ms 455 7.3M 17.0 {YOLOv}5m 640 44.5 44.5 63.1 2.9ms 345 21.4M 51.3 {YOLOv}5l 640 48.1 48.1 66.4 3.8ms 264 47.0M 115.4 {YOLOv}5x 640 50.1 50.1 68.7 6.0ms 167 87.7M 218.8 {YOLOv}5x + {TTA} 832 51.9 51.9 69.6 24.9ms 40 87.7M 1005.3},
	publisher = {Zenodo},
	author = {Glenn Jocher and Alex Stoken and Jirka Borovec and {NanoCode}012 and {ChristopherSTAN} and Liu Changyu and Laughing and tkianai and {yxNONG} and Adam Hogan and lorenzomammana and {AlexWang}1900 and Ayush Chaurasia and Laurentiu Diaconu and Marc and wanghaoyang0106 and ml5ah and Doug and Durgesh and Francisco Ingham and Frederik and Guilhen and Adrien Colmagro and Hu Ye and Jacobsolawetz and Jake Poznanski and Jiacong Fang and Junghoon Kim and Khiem Doan and Lijun Yu 于力军},
	urldate = {2021-02-25},
	date = {2021-01-05},
	doi = {10.5281/zenodo.4418161},
	keywords = {Softwares},
	file = {Zenodo Snapshot:files/47/4418161.html:text/html},
}

@article{gevers_color-based_1999,
	title = {Color-based object recognition},
	pages = {12},
	journaltitle = {Pattern Recognition},
	author = {Gevers, Theo and Smeulders, Arnold W M},
	date = {1999},
	langid = {english},
	file = {Gevers and Smeulders - 1999 - Color-based object recognition.pdf:files/48/Gevers and Smeulders - 1999 - Color-based object recognition.pdf:application/pdf},
}

@article{liu_fast_2012,
	title = {Fast object localization and pose estimation in heavy clutter for robotic bin picking},
	volume = {31},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364911436018},
	doi = {10.1177/0278364911436018},
	abstract = {We present a practical vision-based robotic bin-picking system that performs detection and three-dimensional pose estimation of objects in an unstructured bin using a novel camera design, picks up parts from the bin, and performs error detection and pose correction while the part is in the gripper. Two main innovations enable our system to achieve real-time robust and accurate operation. First, we use a multi-ﬂash camera that extracts robust depth edges. Second, we introduce an efﬁcient shape-matching algorithm called fast directional chamfer matching ({FDCM}), which is used to reliably detect objects and estimate their poses. {FDCM} improves the accuracy of chamfer matching by including edge orientation. It also achieves massive improvements in matching speed using line-segment approximations of edges, a three-dimensional distance transform, and directional integral images. We empirically show that these speedups, combined with the use of bounds in the spatial and hypothesis domains, give the algorithm sublinear computational complexity. We also apply our {FDCM} method to other applications in the context of deformable and articulated shape matching. In addition to signiﬁcantly improving upon the accuracy of previous chamfer matching methods in all of the evaluated applications, {FDCM} is up to two orders of magnitude faster than the previous methods.},
	pages = {951--973},
	number = {8},
	journaltitle = {The International Journal of Robotics Research},
	author = {Liu, Ming-Yu and Tuzel, Oncel and Veeraraghavan, Ashok and Taguchi, Yuichi and Marks, Tim K and Chellappa, Rama},
	urldate = {2021-02-25},
	date = {2012-07},
	langid = {english},
	file = {Liu et al. - 2012 - Fast object localization and pose estimation in he.pdf:files/51/Liu et al. - 2012 - Fast object localization and pose estimation in he.pdf:application/pdf;Liu et al. - 2012 - Fast object localization and pose estimation in he.pdf:files/136/Liu et al. - 2012 - Fast object localization and pose estimation in he.pdf:application/pdf},
}

@incollection{ferrari_making_2018,
	location = {Cham},
	title = {Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose Estimation},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_8},
	abstract = {We introduce a novel method for robust and accurate 3D object pose estimation from single color images under large occlusions. Following recent approaches [1–3], we ﬁrst predict the 2D reprojections of 3D points related to the target object and then compute the 3D pose from these correspondences using a geometric method. Unfortunately, as our experiments show, predicting these 2D reprojections using a regular {CNN} or a Convolutional Pose Machine [4] is very sensitive to partial occlusions, even when these methods are trained with partially occluded examples. Our solution is to predict heatmaps from multiple small patches independently and to accumulate the results to obtain accurate and robust predictions. Training then becomes challenging because patches with similar appearances but diﬀerent positions on the object correspond to diﬀerent heatmaps. However, we provide a simple yet eﬀective solution to deal with such ambiguities. We show that our approach outperforms existing methods on two challenging datasets: The Occluded {LineMOD} dataset, and the {YCB}-Video dataset, both exhibiting cluttered scenes with highly occluded objects.},
	pages = {125--141},
	booktitle = {Computer Vision – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Oberweger, Markus and Rad, Mahdi and Lepetit, Vincent},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	urldate = {2021-02-25},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-030-01267-0_8},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Oberweger et al. - 2018 - Making Deep Heatmaps Robust to Partial Occlusions .pdf:files/52/Oberweger et al. - 2018 - Making Deep Heatmaps Robust to Partial Occlusions .pdf:application/pdf;Oberweger et al. - 2018 - Making Deep Heatmaps Robust to Partial Occlusions .pdf:files/137/Oberweger et al. - 2018 - Making Deep Heatmaps Robust to Partial Occlusions .pdf:application/pdf},
}

@article{matas_robust_2000,
	title = {Robust Detection of Lines Using the Progressive Probabilistic Hough Transform},
	volume = {78},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314299908317},
	doi = {10.1006/cviu.1999.0831},
	pages = {119--137},
	number = {1},
	journaltitle = {Computer Vision and Image Understanding},
	author = {Matas, J. and Galambos, C. and Kittler, J.},
	urldate = {2021-02-25},
	date = {2000-04},
	langid = {english},
	file = {Matas et al. - 2000 - Robust Detection of Lines Using the Progressive Pr.pdf:files/54/Matas et al. - 2000 - Robust Detection of Lines Using the Progressive Pr.pdf:application/pdf;Matas et al. - 2000 - Robust Detection of Lines Using the Progressive Pr.pdf:files/139/Matas et al. - 2000 - Robust Detection of Lines Using the Progressive Pr.pdf:application/pdf},
}

@inproceedings{xiang_posecnn_2018,
	title = {{PoseCNN}: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes},
	isbn = {978-0-9923747-4-7},
	url = {http://www.roboticsproceedings.org/rss14/p19.pdf},
	doi = {10.15607/RSS.2018.XIV.019},
	shorttitle = {{PoseCNN}},
	abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce {PoseCNN}, a new Convolutional Neural Network for 6D object pose estimation. {PoseCNN} estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables {PoseCNN} to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the {YCB}-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the {YCB} dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our {YCBVideo} dataset and the {OccludedLINEMOD} dataset to show that {PoseCNN} is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further reﬁne the poses, our approach achieves state-of-the-art results on the challenging {OccludedLINEMOD} dataset. Our code and dataset are available at https://rse-lab.cs.washington.edu/projects/posecnn/.},
	eventtitle = {Robotics: Science and Systems 2018},
	booktitle = {Robotics: Science and Systems {XIV}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
	urldate = {2021-02-25},
	date = {2018-06-26},
	langid = {english},
	file = {Xiang et al. - 2018 - PoseCNN A Convolutional Neural Network for 6D Obj.pdf:files/56/Xiang et al. - 2018 - PoseCNN A Convolutional Neural Network for 6D Obj.pdf:application/pdf;Xiang et al. - 2018 - PoseCNN A Convolutional Neural Network for 6D Obj.pdf:files/141/Xiang et al. - 2018 - PoseCNN A Convolutional Neural Network for 6D Obj.pdf:application/pdf},
}

@article{papazov_rigid_2012,
	title = {Rigid 3D geometry matching for grasping of known objects in cluttered scenes},
	volume = {31},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364911436019},
	doi = {10.1177/0278364911436019},
	abstract = {In this paper, we present an efﬁcient 3D object recognition and pose estimation approach for grasping procedures in cluttered and occluded environments. In contrast to common appearance-based approaches, we rely solely on 3D geometry information. Our method is based on a robust geometric descriptor, a hashing technique and an efﬁcient, localized {RANSAC}-like sampling strategy. We assume that each object is represented by a model consisting of a set of points with corresponding surface normals. Our method simultaneously recognizes multiple model instances and estimates their pose in the scene. A variety of tests shows that the proposed method performs well on noisy, cluttered and unsegmented range scans in which only small parts of the objects are visible. The main procedure of the algorithm has a linear time complexity resulting in a high recognition speed which allows a direct integration of the method into a continuous manipulation task. The experimental validation with a 7-degrees-of-freedom Cartesian impedance controlled robot shows how the method can be used for grasping objects from a complex random stack. This application demonstrates how the integration of computer vision and softrobotics leads to a robotic system capable of acting in unstructured and occluded environments.},
	pages = {538--553},
	number = {4},
	journaltitle = {The International Journal of Robotics Research},
	author = {Papazov, Chavdar and Haddadin, Sami and Parusel, Sven and Krieger, Kai and Burschka, Darius},
	urldate = {2021-02-25},
	date = {2012-04},
	langid = {english},
	file = {Papazov et al. - 2012 - Rigid 3D geometry matching for grasping of known o.pdf:files/57/Papazov et al. - 2012 - Rigid 3D geometry matching for grasping of known o.pdf:application/pdf;Papazov et al. - 2012 - Rigid 3D geometry matching for grasping of known o.pdf:files/142/Papazov et al. - 2012 - Rigid 3D geometry matching for grasping of known o.pdf:application/pdf},
}

@incollection{hutchison_robust_2013,
	location = {Berlin, Heidelberg},
	title = {Robust and Efficient Pose Estimation from Line Correspondences},
	volume = {7726},
	isbn = {978-3-642-37430-2 978-3-642-37431-9},
	url = {http://link.springer.com/10.1007/978-3-642-37431-9_17},
	abstract = {We propose a non-iterative solution for the Perspective-{nLine} ({PnL}) problem, which can eﬃciently and accurately estimate the camera pose for both small number and large number of line correspondences. By selecting a rotation axis in the camera framework, the reference lines are divided into triplets to form a sixteenth order cost function, and then the optimum is retrieved from the roots of the derivative of the cost function by evaluating the orthogonal errors and the reprojected errors of the local minima. The ﬁnal pose estimation is normalized by a 3D alignment approach. The advantages of the proposed method are as follows: (1) it stably retrieves the optimum of the solution with very little computational complexity and high accuracy; (2) small line sets can be robustly handled to achieve highly accurate results and; (3) large line sets can be eﬃciently handled because it is O(n).},
	pages = {217--230},
	booktitle = {Computer Vision – {ACCV} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Zhang, Lilian and Xu, Chi and Lee, Kok-Meng and Koch, Reinhard},
	editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2021-02-25},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-37431-9_17},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Zhang et al. - 2013 - Robust and Efficient Pose Estimation from Line Cor.pdf:files/58/Zhang et al. - 2013 - Robust and Efficient Pose Estimation from Line Cor.pdf:application/pdf;Zhang et al. - 2013 - Robust and Efficient Pose Estimation from Line Cor.pdf:files/143/Zhang et al. - 2013 - Robust and Efficient Pose Estimation from Line Cor.pdf:application/pdf},
}

@article{deng_self-supervised_2020,
	title = {Self-supervised 6D Object Pose Estimation for Robot Manipulation},
	url = {http://arxiv.org/abs/1909.10159},
	abstract = {To teach robots skills, it is crucial to obtain data with supervision. Since annotating real world data is timeconsuming and expensive, enabling robots to learn in a selfsupervised way is important. In this work, we introduce a robot system for self-supervised 6D object pose estimation. Starting from modules trained in simulation, our system is able to label real world images with accurate 6D object poses for self-supervised learning. In addition, the robot interacts with objects in the environment to change the object conﬁguration by grasping or pushing objects. In this way, our system is able to continuously collect data and improve its pose estimation modules. We show that the self-supervised learning improves object segmentation and 6D pose estimation performance, and consequently enables the system to grasp objects more reliably. A video showing the experiments can be found at https://youtu.be/W1Y0Mmh1Gd8.},
	journaltitle = {{arXiv}:1909.10159 [cs]},
	author = {Deng, Xinke and Xiang, Yu and Mousavian, Arsalan and Eppner, Clemens and Bretl, Timothy and Fox, Dieter},
	urldate = {2021-02-25},
	date = {2020-03-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.10159},
	keywords = {Computer Science - Robotics},
	annotation = {Comment: Accepted to International Conference on Robotics and Automation ({ICRA}), 2020},
	annotation = {Comment: Accepted to International Conference on Robotics and Automation ({ICRA}), 2020},
	file = {Deng et al. - 2020 - Self-supervised 6D Object Pose Estimation for Robo.pdf:files/61/Deng et al. - 2020 - Self-supervised 6D Object Pose Estimation for Robo.pdf:application/pdf;Deng et al. - 2020 - Self-supervised 6D Object Pose Estimation for Robo.pdf:files/146/Deng et al. - 2020 - Self-supervised 6D Object Pose Estimation for Robo.pdf:application/pdf},
}

@article{tsai_new_1989,
	title = {A new technique for fully autonomous and efficient 3D robotics hand/eye calibration},
	volume = {5},
	issn = {1042296X},
	url = {http://ieeexplore.ieee.org/document/34770/},
	doi = {10.1109/70.34770},
	abstract = {This paper describes a new technique for computing 3D position and orientation of a camera relative to the last joint of a robot manipulator in an eye-on-hand configuration. This is part of a trio for real-time 3D robotics eye, eye-to-hand, and hand calibrations, which use a common setup and calibration object, common coordinate systems, matrices, vectors, symbols, and operations throughout the trio, and is especially suited to machine vision community. It is easier and faster than any of the existing techniques, and is ten times more accurate in rotation than any existing technique using standard resolution cameras, and equal to the state-of-the-art vision based technique in terms of linear accuracy. The robot makes a series of automatically planned movements with a camera rigidly mounted at the gripper. At the end of each move, it takes a total of 90 ms to grab an image, extract image feature coordinates, and perform camera extrinsic calibration. After the robot finishes all the movements, it takes only a few milliseconds to do the calibration. A series of generic geometric properties or lemmas are presented, leading to the derivation of the final algorithms, which are aimed at simplicity, efficiency, and accuracy while giving ample geometric and algebraic insights. Besides describing the new technique, critical factors influencing the accuracy are analyzed, and procedures for improving accuracy are introduced. Test results of both simulation and real experiments on an {IBM} Cartesian robot are reported and analyzed.},
	pages = {345--358},
	number = {3},
	journaltitle = {{IEEE} Trans. Robot. Automat.},
	author = {Tsai, R.Y. and Lenz, R.K.},
	urldate = {2021-02-25},
	date = {1989-06},
	langid = {english},
	file = {Tsai and Lenz - 1989 - A new technique for fully autonomous and efficient.pdf:files/64/Tsai and Lenz - 1989 - A new technique for fully autonomous and efficient.pdf:application/pdf;Tsai and Lenz - 1989 - A new technique for fully autonomous and efficient.pdf:files/149/Tsai and Lenz - 1989 - A new technique for fully autonomous and efficient.pdf:application/pdf},
}

@article{wu_integrated_2020,
	title = {An integrated vision-based system for efficient robot arm teleoperation},
	volume = {ahead-of-print},
	issn = {0143-991X, 0143-991X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/IR-06-2020-0129/full/html},
	doi = {10.1108/IR-06-2020-0129},
	abstract = {Purpose – This paper aims to present a natural human–robot teleoperation system, which capitalizes on the latest advancements of monocular human pose estimation to simplify scenario requirements on heterogeneous robot arm teleoperation. Design/methodology/approach – Several optimizations in the joint extraction process are carried on to better balance the performance of the pose estimation network. To bridge the gap between human joint pose in Cartesian space and heterogeneous robot joint angle pose in Radian space, a routinized mapping procedure is proposed.},
	issue = {ahead-of-print},
	journaltitle = {{IR}},
	author = {Wu, Xin and Yang, Canjun and Zhu, Yuanchao and Wu, Weitao and Wei, Qianxiao},
	urldate = {2021-02-25},
	date = {2020-10-12},
	langid = {english},
	file = {Wu et al. - 2020 - An integrated vision-based system for efficient ro.pdf:files/65/Wu et al. - 2020 - An integrated vision-based system for efficient ro.pdf:application/pdf;Wu et al. - 2020 - An integrated vision-based system for efficient ro.pdf:files/150/Wu et al. - 2020 - An integrated vision-based system for efficient ro.pdf:application/pdf},
}

@incollection{yu_3d_2019,
	location = {Cham},
	title = {3D Pose Estimation of Robot Arm with {RGB} Images Based on Deep Learning},
	volume = {11743},
	isbn = {978-3-030-27537-2 978-3-030-27538-9},
	url = {http://link.springer.com/10.1007/978-3-030-27538-9_46},
	abstract = {In the ﬁeld of human-robot interaction, robot collision avoidance with the human in a shared workspace remains a challenge. Many researchers use visual methods to detect the collision between robots and obstacles on the assumption that the robot pose is known because the information about the robot is obtained from the controller and hand-eye calibration is conducted. Therefore, they focus on the motion prediction of obstacles. In this paper, a real-time method based on deep learning is proposed to directly estimate the 3D pose of the robot arm using a color image. The method aims to remove the hand-eye calibration when the system needs to be reconﬁgured and increase the ﬂexibility of the system by eliminating the requirement that the camera ﬁxed relative to the robot. Our approach has two main contributions. One is that the method estimates the 3D position of the robot base and the relative 3D positions of the predeﬁned key points of the robot to the robot base separately different from other deep learning methods considering the limitations of the dataset. The other is that some datasets are collected through another trained network to avoid tedious calibration process, and the trained network will be reused in the pose estimation task. Finally, the experiments are conducted. The results show that a fully trained system provides an accurate 3D pose estimation for the robot arm in the camera coordinate system. The average errors of the 3D positions of the robot base and the predeﬁned key points are 2.35 cm and 1.99 cm respectively.},
	pages = {541--553},
	booktitle = {Intelligent Robotics and Applications},
	publisher = {Springer International Publishing},
	author = {Zhou, Fan and Chi, Zijing and Zhuang, Chungang and Ding, Han},
	editor = {Yu, Haibin and Liu, Jinguo and Liu, Lianqing and Ju, Zhaojie and Liu, Yuwang and Zhou, Dalin},
	urldate = {2021-02-25},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-27538-9_46},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Zhou et al. - 2019 - 3D Pose Estimation of Robot Arm with RGB Images Ba.pdf:files/66/Zhou et al. - 2019 - 3D Pose Estimation of Robot Arm with RGB Images Ba.pdf:application/pdf;Zhou et al. - 2019 - 3D Pose Estimation of Robot Arm with RGB Images Ba.pdf:files/151/Zhou et al. - 2019 - 3D Pose Estimation of Robot Arm with RGB Images Ba.pdf:application/pdf},
}

@inproceedings{ma_edge_2020,
	location = {New York, {NY}, {USA}},
	title = {Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms},
	isbn = {978-1-72816-550-9},
	url = {https://ieeexplore.ieee.org/document/9170983/},
	doi = {10.1109/CSCloud-EdgeCom49738.2020.00050},
	eventtitle = {2020 7th {IEEE} International Conference on Cyber Security and Cloud Computing ({CSCloud})/2020 6th {IEEE} International Conference on Edge Computing and Scalable Cloud ({EdgeCom})},
	pages = {246--251},
	booktitle = {2020 7th {IEEE} International Conference on Cyber Security and Cloud Computing ({CSCloud})/2020 6th {IEEE} International Conference on Edge Computing and Scalable Cloud ({EdgeCom})},
	publisher = {{IEEE}},
	author = {Ma, Qun and Niu, Jianwei and Ouyang, Zhenchao and Li, Mo and Ren, Tao and Li, {QingFeng}},
	urldate = {2021-02-25},
	date = {2020-08},
	langid = {english},
	file = {Ma et al. - 2020 - Edge Computing-based 3D Pose Estimation and Calibr.pdf:files/67/Ma et al. - 2020 - Edge Computing-based 3D Pose Estimation and Calibr.pdf:application/pdf;Ma et al. - 2020 - Edge Computing-based 3D Pose Estimation and Calibr.pdf:files/152/Ma et al. - 2020 - Edge Computing-based 3D Pose Estimation and Calibr.pdf:application/pdf},
}

@article{casado_pose_2017,
	title = {Pose Estimation and Object Tracking Using 2D Images},
	volume = {11},
	issn = {23519789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2351978917303384},
	doi = {10.1016/j.promfg.2017.07.134},
	abstract = {Different factors are forcing a change in the logistics market, most notably e-commerce and manufacturing of custom-made products. This fact is highly linked with warehouse management, where exploitation costs increase with the new value-added tasks required in logistics. In this work, we present a detection system, based on 2D pattern recognition, to estimate the pose of pallets placed in open areas in non-fixed positions. The detection method is part of a novel automation solution designed to retrofit manual operated forklifts, adding a new autonomous working mode, obtaining a highly flexible pallet handling system that could be applied in shared spaces with humans.},
	pages = {63--71},
	journaltitle = {Procedia Manufacturing},
	author = {Casado, Fernando and lapido, Yago Luis and Losada, Diego P. and Santana-Alonso, Alejandro},
	urldate = {2021-02-25},
	date = {2017},
	langid = {english},
	file = {Casado et al. - 2017 - Pose Estimation and Object Tracking Using 2D Image.pdf:files/69/Casado et al. - 2017 - Pose Estimation and Object Tracking Using 2D Image.pdf:application/pdf;Casado et al. - 2017 - Pose Estimation and Object Tracking Using 2D Image.pdf:files/154/Casado et al. - 2017 - Pose Estimation and Object Tracking Using 2D Image.pdf:application/pdf},
}

@article{hameed_pose_2020,
	title = {Pose Estimation of Objects Using Digital Image Processing for Pick-and-Place Applications of Robotic Arms},
	volume = {38},
	issn = {2412-0758, 1681-6900},
	url = {http://engtechjournal.org/index.php/et/article/view/518},
	doi = {10.30684/etj.v38i5A.518},
	abstract = {Robot Vision is one of the most important applications in Image processing. Visual interaction with the environment is a much better way for the robot to gather information and react more intelligently to the variations of the parameters in that environment. A common example of an application that depends on robot vision is that of Pick-And-Place objects by a robotic arm. This work presents a method for identifying an object in a scene and determines its orientation. The method presented enables the robot to choose the best-suited pair of points on the object at which the two-finger gripper can successfully pick the object. The scene is taken by a camera attached to the arm’s end effector which gives 2D images for analysis. The edge detection operation was used to extract a 2D edge image for all the objects in the scene to reduce the time needed for processing. The methods proposed showed accurate object identification which enabled the robotic to successfully identify and pick an object of interest in the scene.},
	pages = {707--718},
	number = {5},
	journaltitle = {{ETJ}},
	author = {Hameed, Firas S. and {Hasan M. Alwan} and {Qasim A. Ateia}},
	urldate = {2021-02-25},
	date = {2020-05-25},
	langid = {english},
	file = {Hameed et al. - 2020 - Pose Estimation of Objects Using Digital Image Pro.pdf:files/70/Hameed et al. - 2020 - Pose Estimation of Objects Using Digital Image Pro.pdf:application/pdf;Hameed et al. - 2020 - Pose Estimation of Objects Using Digital Image Pro.pdf:files/155/Hameed et al. - 2020 - Pose Estimation of Objects Using Digital Image Pro.pdf:application/pdf},
}

@article{kang_eposit_2019,
	title = {{EPOSIT}: An Absolute Pose Estimation Method for Pinhole and Fish-Eye Cameras},
	url = {http://arxiv.org/abs/1909.12945},
	shorttitle = {{EPOSIT}},
	abstract = {This paper presents a generic 6DOF camera pose estimation method, which can be used for both the pinhole camera and the ﬁsh-eye camera. Different from existing methods, relative positions of 3D points rather than absolute coordinates in the world coordinate system are employed in our method, and it has a unique solution. The application scope of {POSIT} (Pose from Orthography and Scaling with Iteration) algorithm is generalized to ﬁsh-eye cameras by combining with the radially symmetric projection model. The image point relationship between the pinhole camera and the ﬁsh-eye camera is derived based on their projection model. The general pose expression which ﬁts for different cameras can be acquired by four noncoplanar object points and their corresponding image points. Accurate estimation results are calculated iteratively. Experimental results on synthetic and real data show that the pose estimation results of our method are more stable and accurate than state-of-the-art methods. The source code is available at https://github.com/k032131/{EPOSIT}.},
	journaltitle = {{arXiv}:1909.12945 [cs]},
	author = {Kang, Zhaobing and Zou, Wei and Zhu, Zheng and Zhang, Chi and Ma, Hongxuan},
	urldate = {2021-02-25},
	date = {2019-09-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.12945},
	keywords = {Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition},
	file = {Kang et al. - 2019 - EPOSIT An Absolute Pose Estimation Method for Pin.pdf:files/71/Kang et al. - 2019 - EPOSIT An Absolute Pose Estimation Method for Pin.pdf:application/pdf;Kang et al. - 2019 - EPOSIT An Absolute Pose Estimation Method for Pin.pdf:files/156/Kang et al. - 2019 - EPOSIT An Absolute Pose Estimation Method for Pin.pdf:application/pdf},
}

@inproceedings{tiwan_cylindrical_2013,
	location = {Pune, India},
	title = {Cylindrical Pellet Pose Estimation in Clutter using a Single Robot Mounted Camera},
	isbn = {978-1-4503-2347-5},
	url = {http://dl.acm.org/citation.cfm?doid=2506095.2506149},
	doi = {10.1145/2506095.2506149},
	abstract = {Pose estimation of cylindrical pellet using a single camera-in-hand configuration of a robot is discussed in this paper. Approaches to estimate pose in both isolated and an occluded environment is discussed. The pellet contour from the segmented image of the scene was compared with contours in the database to ascertain the matching orientation. For occluded pellets, a multiple-view based pose recognition system is proposed. Later, the estimated pose was communicated to the robot to enable it to pick-up the pellet. This has been experimentally implemented for cylindrical pellets and the performance is discussed. The algorithm enables online pellet pose determination and pick-up using {KUKA} {KR}5 robot.},
	eventtitle = {Conference},
	pages = {1--6},
	booktitle = {Proceedings of Conference on Advances In Robotics - {AIR} '13},
	publisher = {{ACM} Press},
	author = {Tiwan, Punit and Boby, Riby Abraham and Roy, Sumantra Dutta and Chaudhury, Santanu and Saha, S. K.},
	urldate = {2021-02-25},
	date = {2013},
	langid = {english},
	file = {Tiwan et al. - 2013 - Cylindrical Pellet Pose Estimation in Clutter usin.pdf:files/74/Tiwan et al. - 2013 - Cylindrical Pellet Pose Estimation in Clutter usin.pdf:application/pdf;Tiwan et al. - 2013 - Cylindrical Pellet Pose Estimation in Clutter usin.pdf:files/159/Tiwan et al. - 2013 - Cylindrical Pellet Pose Estimation in Clutter usin.pdf:application/pdf},
}

@article{ali_multi-view_2020,
	title = {Multi-View Camera Pose Estimation for Robotic Arm Manipulation},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9204616/},
	doi = {10.1109/ACCESS.2020.3026108},
	abstract = {This article proposes a novel approach aimed at estimating the pose of a camera, afﬁxed to a robotic manipulator, against a target object. Our approach provides a way to exploit the redundancy of the robotic arm kinematics by directly considering manipulator poses in the model formulation for camera pose estimation. We adopt a single camera multi-shot technique that minimizes the reprojection error over all the rigid poses. The results of the proposed method are compared to four other studies employing either monocular or stereo setup. The experimental results on synthetic and real data show that the proposed monocular approach achieves better and in some cases comparable results to the stereo approach. Moreover, the proposed approach is signiﬁcantly more robust and precise compared to other methods.},
	pages = {174305--174316},
	journaltitle = {{IEEE} Access},
	author = {Ali, Ihtisham and Suominen, Olli J. and Morales, Emilio Ruiz and Gotchev, Atanas},
	urldate = {2021-02-25},
	date = {2020},
	langid = {english},
	file = {Ali et al. - 2020 - Multi-View Camera Pose Estimation for Robotic Arm .pdf:files/75/Ali et al. - 2020 - Multi-View Camera Pose Estimation for Robotic Arm .pdf:application/pdf;Ali et al. - 2020 - Multi-View Camera Pose Estimation for Robotic Arm .pdf:files/160/Ali et al. - 2020 - Multi-View Camera Pose Estimation for Robotic Arm .pdf:application/pdf},
}

@inproceedings{zeng_robotic_2018,
	location = {Brisbane, {QLD}},
	title = {Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching},
	isbn = {978-1-5386-3081-5},
	url = {https://ieeexplore.ieee.org/document/8461044/},
	doi = {10.1109/ICRA.2018.8461044},
	eventtitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {3750--3757},
	booktitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Zeng, Andy and Song, Shuran and Yu, Kuan-Ting and Donlon, Elliott and Hogan, Francois R. and Bauza, Maria and Ma, Daolin and Taylor, Orion and Liu, Melody and Romo, Eudald and Fazeli, Nima and Alet, Ferran and Dafle, Nikhil Chavan and Holladay, Rachel and Morena, Isabella and Qu Nair, Prem and Green, Druck and Taylor, Ian and Liu, Weber and Funkhouser, Thomas and Rodriguez, Alberto},
	urldate = {2021-02-25},
	date = {2018-05},
	langid = {english},
	file = {Zeng et al. - 2018 - Robotic Pick-and-Place of Novel Objects in Clutter.pdf:files/76/Zeng et al. - 2018 - Robotic Pick-and-Place of Novel Objects in Clutter.pdf:application/pdf;Zeng et al. - 2018 - Robotic Pick-and-Place of Novel Objects in Clutter.pdf:files/161/Zeng et al. - 2018 - Robotic Pick-and-Place of Novel Objects in Clutter.pdf:application/pdf},
}

@inproceedings{chen_random_2018,
	location = {Miyazaki, Japan},
	title = {Random Bin Picking with Multi-view Image Acquisition and {CAD}-Based Pose Estimation},
	isbn = {978-1-5386-6650-0},
	url = {https://ieeexplore.ieee.org/document/8616377/},
	doi = {10.1109/SMC.2018.00381},
	abstract = {Due to the recent development of industrial automation, using vision based techniques to estimate the pose of workpieces for random bin picking application is a future trend. The common method for 3D pose estimation is to use the {CAD} model and feature points. However, the {CAD}-based method has a high degree of ﬂexibility in the assembly line, and it is not easy to acquire all necessary features of the workpieces. In this work, we use two depth cameras to capture the 3D scene, and propose a {CAD}-based multi-view pose estimation algorithm. First, {RANSAC} and an outlier ﬁlter are adopted for noise removal and object segmentation. We use a voting scheme for preliminary pose estimation, followed by the {ICP} algorithm to derive a more precise target pose. Finally, with disturbance detection, the robot arm can grip the objects without rescanning for each operation. A complete system for 3D scene acquisition using structured light cameras, 3D pose estimation and robot arm control is developed for the pick-and-place task. Experiments are carried out in the real scene environment to demonstrate the feasibility of the proposed technique.},
	eventtitle = {2018 {IEEE} International Conference on Systems, Man, and Cybernetics ({SMC})},
	pages = {2218--2223},
	booktitle = {2018 {IEEE} International Conference on Systems, Man, and Cybernetics ({SMC})},
	publisher = {{IEEE}},
	author = {Chen, Yu-Kai and Sun, Guo-Jhen and Lin, Huei-Yung and Chen, Shyh-Leh},
	urldate = {2021-02-25},
	date = {2018-10},
	langid = {english},
	file = {Chen et al. - 2018 - Random Bin Picking with Multi-view Image Acquisiti.pdf:files/77/Chen et al. - 2018 - Random Bin Picking with Multi-view Image Acquisiti.pdf:application/pdf;Chen et al. - 2018 - Random Bin Picking with Multi-view Image Acquisiti.pdf:files/162/Chen et al. - 2018 - Random Bin Picking with Multi-view Image Acquisiti.pdf:application/pdf},
}

@article{song_cad-based_2017,
	title = {{CAD}-based Pose Estimation Design for Random Bin Picking using a {RGB}-D Camera},
	volume = {87},
	issn = {0921-0296, 1573-0409},
	url = {http://link.springer.com/10.1007/s10846-017-0501-1},
	doi = {10.1007/s10846-017-0501-1},
	abstract = {This paper presents a {CAD}-based six-degreesof-freedom (6-{DoF}) pose estimation design for random bin picking for multiple objects. A virtual camera generates a point cloud database for the objects using their 3D {CAD} models. To reduce the computational time of 3D pose estimation, a voxel grid filter reduces the number of points for the 3D cloud of the objects. A voting scheme is used for object recognition and to estimate the 6-{DoF} pose for different objects. An outlier filter filters out badly matching poses so that the robot arm always picks up the upper object in the bin, which increases the success rate. In a computer simulation using a synthetic scene, the average recognition rate is 97.81 \% for three different objects with various poses. A series of experiments have been conducted to validate the proposed method using a Kuka robot arm. The average recognition rate for three objects is 92.39 \% and the picking success rate is 89.67 \%.},
	pages = {455--470},
	number = {3},
	journaltitle = {J Intell Robot Syst},
	author = {Song, Kai-Tai and Wu, Cheng-Hei and Jiang, Sin-Yi},
	urldate = {2021-02-25},
	date = {2017-09},
	langid = {english},
	file = {Song et al. - 2017 - CAD-based Pose Estimation Design for Random Bin Pi.pdf:files/80/Song et al. - 2017 - CAD-based Pose Estimation Design for Random Bin Pi.pdf:application/pdf;Song et al. - 2017 - CAD-based Pose Estimation Design for Random Bin Pi.pdf:files/165/Song et al. - 2017 - CAD-based Pose Estimation Design for Random Bin Pi.pdf:application/pdf},
}

@article{wu_direct_2018,
	title = {Direct pose estimation for planar objects},
	volume = {172},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314218300419},
	doi = {10.1016/j.cviu.2018.03.006},
	abstract = {Estimating six degrees of freedom poses of a planar object from images is an important problem with numerous applications ranging from robotics to augmented reality. While the state-of-the-art Perspective-n-Point algorithms perform well in pose estimation, the success hinges on whether feature points can be extracted and matched correctly on target objects with rich texture. In this work, we propose a two-step robust direct method for six-dimensional pose estimation that performs accurately on both textured and textureless planar target objects. First, the pose of a planar target object with respect to a calibrated camera is approximately estimated by posing it as a template matching problem. Second, each object pose is reﬁned and disambiguated using a dense alignment scheme. Extensive experiments on both synthetic and real datasets demonstrate that the proposed direct pose estimation algorithm performs favorably against state-of-the-art feature-based approaches in terms of robustness and accuracy under varying conditions. Furthermore, we show that the proposed dense alignment scheme can also be used for accurate pose tracking in video sequences.},
	pages = {50--66},
	journaltitle = {Computer Vision and Image Understanding},
	author = {Wu, Po-Chen and Tseng, Hung-Yu and Yang, Ming-Hsuan and Chien, Shao-Yi},
	urldate = {2021-02-25},
	date = {2018-07},
	langid = {english},
	file = {Wu et al. - 2018 - Direct pose estimation for planar objects.pdf:files/83/Wu et al. - 2018 - Direct pose estimation for planar objects.pdf:application/pdf;Wu et al. - 2018 - Direct pose estimation for planar objects.pdf:files/128/Wu et al. - 2018 - Direct pose estimation for planar objects.pdf:application/pdf},
}

@article{horaud_hand-eye_1995,
	title = {Hand-Eye Calibration},
	volume = {14},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/027836499501400301},
	doi = {10.1177/027836499501400301},
	abstract = {Whenever a sensor is mounted on a robot hand it is important to know the relationship between the sensor and the hand. The problem of determining this relationship is referred to as the hand-eye calibration problem. Hand-eye calibration is important in at least two types of tasks: (i) map sensor centered measurements into the robot workspace frame and (ii) allow the robot to precisely move the sensor. In the past some solutions were proposed in the particular case of the sensor being a {TV} camera. With almost no exception, all existing solutions attempt to solve a homogeneous matrix equation of the form {AX} = {XB}. This paper has the following main contributions. First we show that there are two possible formulations of the hand-eye calibration problem. One formulation is the classical one that we just mentioned. A second formulation takes the form of the following homogeneous matrix equation: {MY} = M0Y B. The advantage of the latter formulation is that the extrinsic and intrinsic parameters of the camera need not be made explicit. Indeed, this formulation directly uses the 3 4 perspective matrices (M and M0) associated with 2 positions of the camera with respect to the calibration frame. Moreover, this formulation together with the classical one cover a wider range of camera-based sensors to be calibrated with respect to the robot hand: single scan-line cameras, stereo heads, range nders, etc. Second, we develop a common mathematical framework to solve for the hand-eye calibration problem using either of the two formulations. We represent rotation by a unit quaternion. We present two methods, (i) a closed-form solution for solving for rotation using unit quaternions and then solving for translation and (ii) a non-linear technique for simultaneously solving for rotation and translation. Third, we perform a stability analysis both for our two methods and for the classical linear method developed by Tsai \& Lenz {TL}89]. This analysis allows the comparison of the three methods. In the light of this comparison, the non-linear optimization method, that solves for rotation and translation simultaneously, seems to be the most robust one with respect to noise and to measurement errors.},
	pages = {195--210},
	number = {3},
	journaltitle = {The International Journal of Robotics Research},
	author = {Horaud, Radu and Dornaika, Fadi},
	urldate = {2021-02-25},
	date = {1995-06},
	langid = {english},
	file = {Horaud and Dornaika - 1995 - Hand-Eye Calibration.pdf:files/85/Horaud and Dornaika - 1995 - Hand-Eye Calibration.pdf:application/pdf;Horaud and Dornaika - 1995 - Hand-Eye Calibration.pdf:files/130/Horaud and Dornaika - 1995 - Hand-Eye Calibration.pdf:application/pdf},
}

@inproceedings{hanh_planar_2018,
	location = {Ho Chi Minh City},
	title = {Planar Object Recognition For Bin Picking Application},
	isbn = {978-1-5386-7983-8},
	url = {https://ieeexplore.ieee.org/document/8606884/},
	doi = {10.1109/NICS.2018.8606884},
	abstract = {This paper aims to present a vision-based bin picking system for assembly line in industry. The objects are flat, unique color and occluded each other inside a bin. The whole picking process is divided into two stages. At 3D localization stage, an estimation process using 3D data segment by Euclidean algorithm and calculating surface normal are proposed to estimate angle and position of an object. To reduce the burden time of 3D pose estimation, a voxel grid filter is implemented to reduce the number of points for the 3D cloud of the objects. As known the 3D image in bin often involves both heavy noise and edge distortions, so to prepare for the assembly a 5DOF robot will pick and place it in 2D table then an 2D camera is used to estimate the pose of the object correctly. To prove the efficiency of proposed system that can pick up all objects in the bin a series of experiments on a 6-axis robot are implemented.},
	eventtitle = {2018 5th {NAFOSTED} Conference on Information and Computer Science ({NICS})},
	pages = {211--215},
	booktitle = {2018 5th {NAFOSTED} Conference on Information and Computer Science ({NICS})},
	publisher = {{IEEE}},
	author = {Hanh, Le Duc and Duc, Le Minh},
	urldate = {2021-02-25},
	date = {2018-11},
	langid = {english},
	file = {Hanh and Duc - 2018 - Planar Object Recognition For Bin Picking Applicat.pdf:files/86/Hanh and Duc - 2018 - Planar Object Recognition For Bin Picking Applicat.pdf:application/pdf;Hanh and Duc - 2018 - Planar Object Recognition For Bin Picking Applicat.pdf:files/131/Hanh and Duc - 2018 - Planar Object Recognition For Bin Picking Applicat.pdf:application/pdf},
}

@inproceedings{horanyi_generalized_2017,
	location = {Qingdao},
	title = {Generalized Pose Estimation from Line Correspondences with Known Vertical Direction},
	isbn = {978-1-5386-2610-8},
	url = {https://ieeexplore.ieee.org/document/8374577/},
	doi = {10.1109/3DV.2017.00036},
	abstract = {We propose a novel method to compute the absolute pose of a generalized camera based on straight lines, which are common in urban environment. The only assumption about the imaging model is that 3D straight lines are projected via projection planes determined by the line and camera projection directions, i.e. correspondences are given as a 3D world line and its projection plane. Since modern cameras are frequently equipped with various location and orientation sensors, we assume that the vertical direction (e.g. a gravity vector) is available. Therefore we formulate the problem in terms of 4 unknowns using 3D line - projection plane correspondences which yields a closed form solution. The solution can be used as a minimal solver as well as a least squares solver without reformulation. The proposed algorithm have been evaluated on various synthetic datasets as well as on real data. Experimental results conﬁrm state of the art performance both in terms of quality and computing time.},
	eventtitle = {2017 International Conference on 3D Vision (3DV)},
	pages = {244--253},
	booktitle = {2017 International Conference on 3D Vision (3DV)},
	publisher = {{IEEE}},
	author = {Horanyi, Nora and Kato, Zoltan},
	urldate = {2021-02-25},
	date = {2017-10},
	langid = {english},
	file = {Horanyi and Kato - 2017 - Generalized Pose Estimation from Line Corresponden.pdf:files/87/Horanyi and Kato - 2017 - Generalized Pose Estimation from Line Corresponden.pdf:application/pdf;Horanyi and Kato - 2017 - Generalized Pose Estimation from Line Corresponden.pdf:files/132/Horanyi and Kato - 2017 - Generalized Pose Estimation from Line Corresponden.pdf:application/pdf},
}

@inproceedings{vogt_system_2017,
	location = {Singapore, Singapore},
	title = {A system for learning continuous human-robot interactions from human-human demonstrations},
	isbn = {978-1-5090-4633-1},
	url = {http://ieeexplore.ieee.org/document/7989334/},
	doi = {10.1109/ICRA.2017.7989334},
	abstract = {We present a data-driven imitation learning system for learning human-robot interactions from human-human demonstrations. During training, the movements of two interaction partners are recorded through motion capture and an interaction model is learned. At runtime, the interaction model is used to continuously adapt the robot’s motion, both spatially and temporally, to the movements of the human interaction partner. We show the effectiveness of the approach on complex, sequential tasks by presenting two applications involving collaborative human-robot assembly. Experiments with varied object hand-over positions and task execution speeds conﬁrm the capabilities for spatio-temporal adaption of the demonstrated behavior to the current situation.},
	eventtitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {2882--2889},
	booktitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Vogt, David and Stepputtis, Simon and Grehl, Steve and Jung, Bernhard and Ben Amor, Heni},
	urldate = {2021-02-25},
	date = {2017-05},
	langid = {english},
	file = {Vogt et al. - 2017 - A system for learning continuous human-robot inter.pdf:files/133/Vogt et al. - 2017 - A system for learning continuous human-robot inter.pdf:application/pdf},
}

@article{nadeau_impedance_2019,
	title = {Impedance Control Self-Calibration of a Collaborative Robot Using Kinematic Coupling},
	volume = {8},
	issn = {2218-6581},
	url = {https://www.mdpi.com/2218-6581/8/2/33},
	doi = {10.3390/robotics8020033},
	abstract = {This paper presents a closed-loop calibration approach using impedance control. The process is managed by a data communication architecture based on open-source tools and designed for adaptability. The calibration procedure uses precision spheres and a kinematic coupling standard machine tool components, which are suitable for harsh industrial environments. As such, the required equipment is low cost (approximately \$2000 {USD}), robust, and is quick to set up, especially when compared to traditional calibration devices. As demonstrated through an experimental study and validated with a laser tracker, the absolute accuracy of the {KUKA} {LBR} iiwa robot was improved to a maximum error of 0.990 mm, representing a 58.4\% improvement when compared to the nominal model. Further testing showed that a traditional calibration using a laser tracker only improved the maximum error by 58 µm over the impedance control approach.},
	pages = {33},
	number = {2},
	journaltitle = {Robotics},
	author = {Nadeau, Nicholas A. and Bonev, Ilian A. and Joubair, Ahmed},
	urldate = {2021-02-25},
	date = {2019-04-23},
	langid = {english},
	file = {Nadeau et al. - 2019 - Impedance Control Self-Calibration of a Collaborat.pdf:files/134/Nadeau et al. - 2019 - Impedance Control Self-Calibration of a Collaborat.pdf:application/pdf;Nadeau et al. - 2019 - Impedance Control Self-Calibration of a Collaborat.pdf:files/140/Nadeau et al. - 2019 - Impedance Control Self-Calibration of a Collaborat.pdf:application/pdf},
}

@article{hayat_geometric_2019,
	title = {A geometric approach for kinematic identification of an industrial robot using a monocular camera},
	volume = {57},
	issn = {07365845},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584517304234},
	doi = {10.1016/j.rcim.2018.11.008},
	abstract = {We propose a generic formulation to identify the kinematic parameters of an industrial robot using a geometric approach when no prior information about the robot’s kinematics is available. The joint axes were estimated using the singular value decomposition applied to the pose data of the robot’s end-eﬀector. These data were obtained by actuating one joint of the robot at a time. The approach is ﬁrst illustrated using the {CAD} model of the robot. Next, a simulation study was performed to decide on the number of data points and angular actuation required by a revolute joint to estimate the kinematic parameters. This was done considering the fact that there exist noise in the measurements from the sensors.},
	pages = {329--346},
	journaltitle = {Robotics and Computer-Integrated Manufacturing},
	author = {Hayat, Abdullah Aamir and Boby, Riby Abraham and Saha, Subir Kumar},
	urldate = {2021-02-25},
	date = {2019-06},
	langid = {english},
	file = {Hayat et al. - 2019 - A geometric approach for kinematic identification .pdf:files/138/Hayat et al. - 2019 - A geometric approach for kinematic identification .pdf:application/pdf},
}

@inproceedings{boby_hand-eye_2020,
	location = {Innopolis, Russia},
	title = {Hand-eye calibration using a single image and robotic picking up using images lacking in contrast},
	isbn = {978-1-72818-763-1},
	url = {https://ieeexplore.ieee.org/document/9290197/},
	doi = {10.1109/NIR50484.2020.9290197},
	abstract = {This article proposes a hand-eye calibration using a new and easy method suitable for a camera mounted on the end-effector of an industrial robot using only a single image. The hand-eye calibration information could be used in robotic picking up of cubes using a monocular camera. Images captured from a particular pose of the camera have been segmented using a fusion of multiple methods such that the object information is obtained even in cases when there is less contrast between the object and the background, or in the presence of variation in lighting. The edge information, and subsequently the pose of the object was estimated using minimum number of images. In some of the cases a single image was sufﬁcient but in case only a single edge edge is obtained, an additional image is grabbed after aligning the camera with the detected edge. An additional edge is estimated using a directional thresholding operation. The edge information in 3-D obtained using the calibration information was then used to calculate the pose of the object to facilitate robotic pick up. To ensure safety; a veriﬁcation of the estimate was done using projection of the computed coordinates, and ﬁnal pick up was done while monitoring the force to avoid damage due to collisions. The proposed approaches were physically implemented and experimentally validated.},
	eventtitle = {2020 International Conference "Nonlinearity, Information and Robotics" ({NIR})},
	pages = {1--6},
	booktitle = {2020 International Conference Nonlinearity, Information and Robotics ({NIR})},
	publisher = {{IEEE}},
	author = {Boby, Riby Abraham},
	urldate = {2021-02-25},
	date = {2020-12-03},
	langid = {english},
	file = {Boby - 2020 - Hand-eye calibration using a single image and robo.pdf:files/144/Boby - 2020 - Hand-eye calibration using a single image and robo.pdf:application/pdf},
}

@article{shih_simple_2018,
	title = {A Simple Robotic Eye-In-Hand Camera Positioning and Alignment Control Method Based on Parallelogram Features},
	volume = {7},
	issn = {2218-6581},
	url = {http://www.mdpi.com/2218-6581/7/2/31},
	doi = {10.3390/robotics7020031},
	abstract = {A simple and effective method for camera positioning and alignment control for robotic pick-and-place tasks is described here. A parallelogram feature is encoded into each 3D object or target location. To determine the pose of each part and guide the robot precisely, a camera is mounted on the robot end-ﬂange to determine and measure the location and pose of the part. The robot then adjusts the camera to align it with the located part so that it can be grasped. The overall robot control system follows a continuous look-and-move control strategy. After a position-based coarse alignment, a sequence of image-based ﬁne alignment steps is carried out, and the part is then picked and placed by the robot arm gripper. Experimental results showed an excellent applicability of the proposed approach for pick-and-place tasks, and the overall errors were 1.2 mm for positioning and 1.0◦ for orientation angle.},
	pages = {31},
	number = {2},
	journaltitle = {Robotics},
	author = {Shih, Ching-Long and Lee, Yi},
	urldate = {2021-02-25},
	date = {2018-06-18},
	langid = {english},
	file = {Shih and Lee - 2018 - A Simple Robotic Eye-In-Hand Camera Positioning an.pdf:files/145/Shih and Lee - 2018 - A Simple Robotic Eye-In-Hand Camera Positioning an.pdf:application/pdf},
}

@inproceedings{gamez_garcia_sensor_2004,
	location = {Sendai, Japan},
	title = {Sensor fusion of force and acceleration for robot force control},
	volume = {3},
	isbn = {978-0-7803-8463-7},
	url = {http://ieeexplore.ieee.org/document/1389867/},
	doi = {10.1109/IROS.2004.1389867},
	abstract = {Absfrocf-In this paper, robotic sensor fusion of acceleration and force measurement is considered. We discuss the problem of using accelerometers close l o the end-effedors of robotic manipulators and how it may improve the force contml performance. We introduce B new model-based observer appmaeh to sensor fusion of information fmm vmious dillerent sensors. During contact transition, accelerometers and force sensors play a very important role and it can overcome many of the difficulties of uncertain models and unknown emironments, which limit the domain of application of currents robots used without external sensory feedback. A model of the robot-grinding tool using the new sensors was obtained by system identification. A n impedance control scheme was proposed to veriry the improvement The experiments were carried out on an {ABB} industrial robot with open control system architedure.},
	eventtitle = {2004 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS}) ({IEEE} Cat. No.04CH37566)},
	pages = {3009--3014},
	booktitle = {2004 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS}) ({IEEE} Cat. No.04CH37566)},
	publisher = {{IEEE}},
	author = {Gamez Garcia, J. and Robertsson, A. and Gomez Ortega, J. and Johansson, R.},
	urldate = {2021-02-25},
	date = {2004},
	langid = {english},
	file = {Gamez Garcia et al. - 2004 - Sensor fusion of force and acceleration for robot .pdf:files/147/Gamez Garcia et al. - 2004 - Sensor fusion of force and acceleration for robot .pdf:application/pdf},
}

@inproceedings{chawda_toward_2017,
	location = {Vancouver, {BC}},
	title = {Toward torque control of a {KUKA} {LBR} {IIWA} for physical human-robot interaction},
	isbn = {978-1-5386-2682-5},
	url = {http://ieeexplore.ieee.org/document/8206543/},
	doi = {10.1109/IROS.2017.8206543},
	abstract = {In this paper we examine joint torque tracking as well as estimation of external torques for the {KUKA} Lightweight Robot ({LBR}) {IIWA}. To support physical humanrobot interaction tasks, we need smooth estimation that allows detection of delicate external events and good control to hide inertial forces. Unfortunately a transmission nonlinearity in the motor to joint gearing injects vibrations and limits the performance of the built-in torque controller and observer. We conﬁrm the nonlinearity to be a spatially periodic deﬂection between the motor and joint. Identiﬁcation of this behavior allows us to generate more accurate joint position measurements. We also design a matching spatial ﬁlter to remove the vibrations from joint torque measurements. Experiments on an {LBR} {IIWA} show that compensating for the nonlinearity provides smoother external torque estimates and improves the torque tracking performance. Furthermore, we are able to increase the gain margin more than three fold over the built-in controller.},
	eventtitle = {2017 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {6387--6392},
	booktitle = {2017 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Chawda, Vinay and Niemeyer, Gunter},
	urldate = {2021-02-25},
	date = {2017-09},
	langid = {english},
	file = {Chawda and Niemeyer - 2017 - Toward torque control of a KUKA LBR IIWA for physi.pdf:files/148/Chawda and Niemeyer - 2017 - Toward torque control of a KUKA LBR IIWA for physi.pdf:application/pdf},
}

@inproceedings{boby_single_2016,
	location = {Stockholm, Sweden},
	title = {Single image based camera calibration and pose estimation of the end-effector of a robot},
	isbn = {978-1-4673-8026-3},
	url = {http://ieeexplore.ieee.org/document/7487395/},
	doi = {10.1109/ICRA.2016.7487395},
	abstract = {A new method is proposed for measurement of six dimensional pose of an industrial robot using a single image from a camera which is not pre-calibrated. Additionally during the pose determination, camera internal parameters are also obtained, which makes the method a suitable alternative for calibrating camera using a single image. Results from the two variants of the proposed approach are compared with Zhang's camera calibration algorithm and has been found to be better. Another utility of the proposed algorithm is to measure six dimensional pose of an industrial robot. Robot repeatability was also measured using the proposed camera calibration algorithm. The repeatability results were compared with the measurements using Artificial Reality toolkit {ArUco} which is a de-facto standard in the pose measurements using camera. The performance of the proposed method is better than that obtained from {ArUco}. Another area of application is identification of kinematic parameters. Using the circle point analysis method, identification of {KUKA} {KR}5 Arc robot was done using the proposed method.},
	eventtitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {2435--2440},
	booktitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Boby, R. A. and Saha, S. K.},
	urldate = {2021-02-25},
	date = {2016-05},
	langid = {english},
	file = {Boby and Saha - 2016 - Single image based camera calibration and pose est.pdf:files/153/Boby and Saha - 2016 - Single image based camera calibration and pose est.pdf:application/pdf},
}

@inproceedings{sharma_projectable_2015,
	location = {Nagoya, Japan},
	title = {Projectable interactive surface using microsoft kinect V2: Recovering information from coarse data to detect touch},
	isbn = {978-1-4673-7242-8},
	url = {http://ieeexplore.ieee.org/document/7405081/},
	doi = {10.1109/SII.2015.7405081},
	shorttitle = {Projectable interactive surface using microsoft kinect V2},
	abstract = {An Image-projective Desktop Varnamala Trainer ({IDVT}) called {SAKSHAR} has been designed to improve the learning by children through an interactive audio visual feedback system. This device uses a projector to render a virtual display, which permits production of large interactive   !   \&! \% "            !"      \#!  (!      is recognized with the help of a Microsoft Kinect Version 2. The entire system is portable, i.e., it can be projected on any planar surface. Since the Kinect does not give precise 3D coordinates of points for detecting a touch, a model of recognition of a touch purely   !      "      "  "    "   \#!  (!      \% "  "   !\#      \% \#    not yield accurate results. We have instead modeled the touch action by using multiple points along the trajectory of the "           "    "   \#!  (!      \%              !    "  " with the surface. Fitting a curve through these points and analyzing the errors is used to make the detection of touch accurate.},
	eventtitle = {2015 {IEEE}/{SICE} International Symposium on System Integration ({SII})},
	pages = {795--800},
	booktitle = {2015 {IEEE}/{SICE} International Symposium on System Integration ({SII})},
	publisher = {{IEEE}},
	author = {Sharma, P. and Joshi, R. P. and Boby, R. A. and Saha, S. K. and Matsumaru, T.},
	urldate = {2021-02-25},
	date = {2015-12},
	langid = {english},
	file = {Sharma et al. - 2015 - Projectable interactive surface using microsoft ki.pdf:files/157/Sharma et al. - 2015 - Projectable interactive surface using microsoft ki.pdf:application/pdf},
}

@inproceedings{jain_repeatability_2019,
	location = {Chennai India},
	title = {Repeatability measurement and kinematic identification of {LBR} iiwa 7 R800 using monocular camera},
	isbn = {978-1-4503-6650-2},
	url = {http://dl.acm.org/doi/10.1145/3352593.3352617},
	doi = {10.1145/3352593.3352617},
	abstract = {In this paper, we have performed the kinematic identification and repeatability analysis of {LBR} iiwa 7 R800 (7 axis serial link robot) using monocular camera mounted at the end-effector of the robot. We started the process with the camera calibration process to identify intrinsic and extrinsic parameters of the camera used. In order to determine the pose of the end-effector using camera for repeatability analysis, we have used a 9x6 checkerboard for the repeatability experiment and for kinematic identification we have used {ArUco} markers. For repeatability analysis, we have used poses from {ISO} 9283 standards. Also we have used dispersion as a statistical means for quantifying the repeatability analysis. Subsequently, we have compared the results of kinematic identification with those from laser sensors and the theoretical {CAD} data sheet provided for the robot. Also in this paper, the algorithm has been introduced for measuring repeatability under force control mode and consequently, a single point repeatabilty has been evaluated.},
	eventtitle = {{AIR} 2019: Advances in Robotics 2019},
	pages = {1--6},
	booktitle = {Proceedings of the Advances in Robotics 2019},
	publisher = {{ACM}},
	author = {Jain, Aditya and Singh, Hardeep and Boby, Riby Abraham and Saha, Subir Kumar and Kumar, Swagat and Roy, Sumantra Dutta},
	urldate = {2021-02-25},
	date = {2019-07-02},
	langid = {english},
	file = {Jain et al. - 2019 - Repeatability measurement and kinematic identifica.pdf:files/158/Jain et al. - 2019 - Repeatability measurement and kinematic identifica.pdf:application/pdf},
}

@inproceedings{buchholz_efficient_2013,
	location = {Karlsruhe, Germany},
	title = {Efficient bin-picking and grasp planning based on depth data},
	isbn = {978-1-4673-5643-5 978-1-4673-5641-1},
	url = {http://ieeexplore.ieee.org/document/6631029/},
	doi = {10.1109/ICRA.2013.6631029},
	abstract = {The problem of object localization is a well-known problem in industrial robotics. Manufactured parts arrive at factories as bulk goods in boxes. Single parts need to be picked out of the boxes and have to be fed to a machine. The task of automatically isolating single objects is known as the binpicking problem. Even in modern factories the task of binpicking is not automated widely yet. The automatization of this task is expensive since state-of-the-art solutions require object-class speciﬁc algorithms. In this paper we present an applicable solution for the bin-picking problem which is based on a standard 3d-sensor and is able to handle arbitrary objects. Furthermore, it is robust against noise and object occlusions. Additionally, we propose an approach for optimal grasp pose estimation with collision avoidance that effectively reduces system cycle times.},
	eventtitle = {2013 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {3245--3250},
	booktitle = {2013 {IEEE} International Conference on Robotics and Automation},
	publisher = {{IEEE}},
	author = {Buchholz, Dirk and Futterlieb, Marcus and Winkelbach, Simon and Wahl, Friedrich M.},
	urldate = {2021-02-25},
	date = {2013-05},
	langid = {english},
	file = {Buchholz et al. - 2013 - Efficient bin-picking and grasp planning based on .pdf:files/163/Buchholz et al. - 2013 - Efficient bin-picking and grasp planning based on .pdf:application/pdf;Buchholz et al. - 2013 - Efficient bin-picking and grasp planning based on .pdf:files/164/Buchholz et al. - 2013 - Efficient bin-picking and grasp planning based on .pdf:application/pdf},
}

@inproceedings{briot_situ_2014,
	location = {Besacon},
	title = {In situ calibration of joint torque sensors of the {KUKA} {LightWeight} robot using only internal controller data},
	isbn = {978-1-4799-5736-1 978-1-4799-5735-4},
	url = {http://ieeexplore.ieee.org/document/6878122/},
	doi = {10.1109/AIM.2014.6878122},
	abstract = {The Kuka {LWR} is equipped with torque sensors mounted into the actuated joints. Each torque sensor is calibrated separately before it is mounted on the robot. This needs a second calibration at the last stage of the assembling of the robot in order to take into account the effect of the robot structure through it's jacobian matrix. This final calibration is necessary to improve the accuracy of the estimation of the interaction wrench of the robot with its environment. However, the proposed calibration techniques are usually complicated, time-consuming, and must be carried out before assembling the sensors on the robot. In this paper, a simple and fast method for calibrating the sensors once they are assembled on the robot is presented. The method is based on the least squares solution of an over-determined linear system obtained with the robot inverse dynamic identification model in which are included the sensor gains. This model is calculated with available sensor measurement and joint position sampled data while the robot is tracking some reference trajectories without load on the robot and some trajectories with a known payload fixed on the robot. The method is experimentally validated on the Kuka {LWR}4+ but can be applied to any similar kind of robot equipped with joint torque sensors.},
	eventtitle = {2014 {IEEE}/{ASME} International Conference on Advanced Intelligent Mechatronics ({AIM})},
	pages = {470--475},
	booktitle = {2014 {IEEE}/{ASME} International Conference on Advanced Intelligent Mechatronics},
	publisher = {{IEEE}},
	author = {Briot, S. and Gautier, M. and Jubien, A.},
	urldate = {2021-02-25},
	date = {2014-07},
	langid = {english},
	file = {Briot et al. - 2014 - In situ calibration of joint torque sensors of the.pdf:files/166/Briot et al. - 2014 - In situ calibration of joint torque sensors of the.pdf:application/pdf},
}