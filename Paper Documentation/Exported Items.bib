
@inproceedings{vogt_system_2017,
	location = {Singapore, Singapore},
	title = {A system for learning continuous human-robot interactions from human-human demonstrations},
	isbn = {978-1-5090-4633-1},
	url = {http://ieeexplore.ieee.org/document/7989334/},
	doi = {10.1109/ICRA.2017.7989334},
	abstract = {We present a data-driven imitation learning system for learning human-robot interactions from human-human demonstrations. During training, the movements of two interaction partners are recorded through motion capture and an interaction model is learned. At runtime, the interaction model is used to continuously adapt the robot’s motion, both spatially and temporally, to the movements of the human interaction partner. We show the effectiveness of the approach on complex, sequential tasks by presenting two applications involving collaborative human-robot assembly. Experiments with varied object hand-over positions and task execution speeds conﬁrm the capabilities for spatio-temporal adaption of the demonstrated behavior to the current situation.},
	eventtitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {2882--2889},
	booktitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Vogt, David and Stepputtis, Simon and Grehl, Steve and Jung, Bernhard and Ben Amor, Heni},
	urldate = {2021-02-24},
	date = {2017-05},
	langid = {english},
	file = {Vogt et al. - 2017 - A system for learning continuous human-robot inter.pdf:C\:\\Users\\mosta\\Zotero\\storage\\MFDILTAF\\Vogt et al. - 2017 - A system for learning continuous human-robot inter.pdf:application/pdf},
}

@article{nadeau_impedance_2019,
	title = {Impedance Control Self-Calibration of a Collaborative Robot Using Kinematic Coupling},
	volume = {8},
	issn = {2218-6581},
	url = {https://www.mdpi.com/2218-6581/8/2/33},
	doi = {10.3390/robotics8020033},
	abstract = {This paper presents a closed-loop calibration approach using impedance control. The process is managed by a data communication architecture based on open-source tools and designed for adaptability. The calibration procedure uses precision spheres and a kinematic coupling standard machine tool components, which are suitable for harsh industrial environments. As such, the required equipment is low cost (approximately \$2000 {USD}), robust, and is quick to set up, especially when compared to traditional calibration devices. As demonstrated through an experimental study and validated with a laser tracker, the absolute accuracy of the {KUKA} {LBR} iiwa robot was improved to a maximum error of 0.990 mm, representing a 58.4\% improvement when compared to the nominal model. Further testing showed that a traditional calibration using a laser tracker only improved the maximum error by 58 µm over the impedance control approach.},
	pages = {33},
	number = {2},
	journaltitle = {Robotics},
	shortjournal = {Robotics},
	author = {Nadeau, Nicholas A. and Bonev, Ilian A. and Joubair, Ahmed},
	urldate = {2021-02-24},
	date = {2019-04-23},
	langid = {english},
	file = {Nadeau et al. - 2019 - Impedance Control Self-Calibration of a Collaborat.pdf:C\:\\Users\\mosta\\Zotero\\storage\\PRUCJB7B\\Nadeau et al. - 2019 - Impedance Control Self-Calibration of a Collaborat.pdf:application/pdf},
}

@inproceedings{boby_hand-eye_2020,
	location = {Innopolis, Russia},
	title = {Hand-eye calibration using a single image and robotic picking up using images lacking in contrast},
	isbn = {978-1-72818-763-1},
	url = {https://ieeexplore.ieee.org/document/9290197/},
	doi = {10.1109/NIR50484.2020.9290197},
	abstract = {This article proposes a hand-eye calibration using a new and easy method suitable for a camera mounted on the end-effector of an industrial robot using only a single image. The hand-eye calibration information could be used in robotic picking up of cubes using a monocular camera. Images captured from a particular pose of the camera have been segmented using a fusion of multiple methods such that the object information is obtained even in cases when there is less contrast between the object and the background, or in the presence of variation in lighting. The edge information, and subsequently the pose of the object was estimated using minimum number of images. In some of the cases a single image was sufﬁcient but in case only a single edge edge is obtained, an additional image is grabbed after aligning the camera with the detected edge. An additional edge is estimated using a directional thresholding operation. The edge information in 3-D obtained using the calibration information was then used to calculate the pose of the object to facilitate robotic pick up. To ensure safety; a veriﬁcation of the estimate was done using projection of the computed coordinates, and ﬁnal pick up was done while monitoring the force to avoid damage due to collisions. The proposed approaches were physically implemented and experimentally validated.},
	eventtitle = {2020 International Conference "Nonlinearity, Information and Robotics" ({NIR})},
	pages = {1--6},
	booktitle = {2020 International Conference Nonlinearity, Information and Robotics ({NIR})},
	publisher = {{IEEE}},
	author = {Boby, Riby Abraham},
	urldate = {2021-02-24},
	date = {2020-12-03},
	langid = {english},
	file = {Boby - 2020 - Hand-eye calibration using a single image and robo.pdf:C\:\\Users\\mosta\\Zotero\\storage\\LRDUMVNK\\Boby - 2020 - Hand-eye calibration using a single image and robo.pdf:application/pdf},
}

@article{shih_simple_2018,
	title = {A Simple Robotic Eye-In-Hand Camera Positioning and Alignment Control Method Based on Parallelogram Features},
	volume = {7},
	issn = {2218-6581},
	url = {http://www.mdpi.com/2218-6581/7/2/31},
	doi = {10.3390/robotics7020031},
	abstract = {A simple and effective method for camera positioning and alignment control for robotic pick-and-place tasks is described here. A parallelogram feature is encoded into each 3D object or target location. To determine the pose of each part and guide the robot precisely, a camera is mounted on the robot end-ﬂange to determine and measure the location and pose of the part. The robot then adjusts the camera to align it with the located part so that it can be grasped. The overall robot control system follows a continuous look-and-move control strategy. After a position-based coarse alignment, a sequence of image-based ﬁne alignment steps is carried out, and the part is then picked and placed by the robot arm gripper. Experimental results showed an excellent applicability of the proposed approach for pick-and-place tasks, and the overall errors were 1.2 mm for positioning and 1.0◦ for orientation angle.},
	pages = {31},
	number = {2},
	journaltitle = {Robotics},
	shortjournal = {Robotics},
	author = {Shih, Ching-Long and Lee, Yi},
	urldate = {2021-02-24},
	date = {2018-06-18},
	langid = {english},
	file = {Shih and Lee - 2018 - A Simple Robotic Eye-In-Hand Camera Positioning an.pdf:C\:\\Users\\mosta\\Zotero\\storage\\SML3C9IT\\Shih and Lee - 2018 - A Simple Robotic Eye-In-Hand Camera Positioning an.pdf:application/pdf},
}

@inproceedings{gamez_garcia_sensor_2004,
	location = {Sendai, Japan},
	title = {Sensor fusion of force and acceleration for robot force control},
	volume = {3},
	isbn = {978-0-7803-8463-7},
	url = {http://ieeexplore.ieee.org/document/1389867/},
	doi = {10.1109/IROS.2004.1389867},
	abstract = {Absfrocf-In this paper, robotic sensor fusion of acceleration and force measurement is considered. We discuss the problem of using accelerometers close l o the end-effedors of robotic manipulators and how it may improve the force contml performance. We introduce B new model-based observer appmaeh to sensor fusion of information fmm vmious dillerent sensors. During contact transition, accelerometers and force sensors play a very important role and it can overcome many of the difficulties of uncertain models and unknown emironments, which limit the domain of application of currents robots used without external sensory feedback. A model of the robot-grinding tool using the new sensors was obtained by system identification. A n impedance control scheme was proposed to veriry the improvement The experiments were carried out on an {ABB} industrial robot with open control system architedure.},
	eventtitle = {2004 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS}) ({IEEE} Cat. No.04CH37566)},
	pages = {3009--3014},
	booktitle = {2004 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS}) ({IEEE} Cat. No.04CH37566)},
	publisher = {{IEEE}},
	author = {Gamez Garcia, J. and Robertsson, A. and Gomez Ortega, J. and Johansson, R.},
	urldate = {2021-02-24},
	date = {2004},
	langid = {english},
	file = {Gamez Garcia et al. - 2004 - Sensor fusion of force and acceleration for robot .pdf:C\:\\Users\\mosta\\Zotero\\storage\\D9D5423J\\Gamez Garcia et al. - 2004 - Sensor fusion of force and acceleration for robot .pdf:application/pdf},
}

@inproceedings{chawda_toward_2017,
	location = {Vancouver, {BC}},
	title = {Toward torque control of a {KUKA} {LBR} {IIWA} for physical human-robot interaction},
	isbn = {978-1-5386-2682-5},
	url = {http://ieeexplore.ieee.org/document/8206543/},
	doi = {10.1109/IROS.2017.8206543},
	abstract = {In this paper we examine joint torque tracking as well as estimation of external torques for the {KUKA} Lightweight Robot ({LBR}) {IIWA}. To support physical humanrobot interaction tasks, we need smooth estimation that allows detection of delicate external events and good control to hide inertial forces. Unfortunately a transmission nonlinearity in the motor to joint gearing injects vibrations and limits the performance of the built-in torque controller and observer. We conﬁrm the nonlinearity to be a spatially periodic deﬂection between the motor and joint. Identiﬁcation of this behavior allows us to generate more accurate joint position measurements. We also design a matching spatial ﬁlter to remove the vibrations from joint torque measurements. Experiments on an {LBR} {IIWA} show that compensating for the nonlinearity provides smoother external torque estimates and improves the torque tracking performance. Furthermore, we are able to increase the gain margin more than three fold over the built-in controller.},
	eventtitle = {2017 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {6387--6392},
	booktitle = {2017 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Chawda, Vinay and Niemeyer, Gunter},
	urldate = {2021-02-24},
	date = {2017-09},
	langid = {english},
	file = {Chawda and Niemeyer - 2017 - Toward torque control of a KUKA LBR IIWA for physi.pdf:C\:\\Users\\mosta\\Zotero\\storage\\9ZUXBD5X\\Chawda and Niemeyer - 2017 - Toward torque control of a KUKA LBR IIWA for physi.pdf:application/pdf},
}

@inproceedings{boby_single_2016,
	location = {Stockholm, Sweden},
	title = {Single image based camera calibration and pose estimation of the end-effector of a robot},
	isbn = {978-1-4673-8026-3},
	url = {http://ieeexplore.ieee.org/document/7487395/},
	doi = {10.1109/ICRA.2016.7487395},
	abstract = {A new method is proposed for measurement of six dimensional pose of an industrial robot using a single image from a camera which is not pre-calibrated. Additionally during the pose determination, camera internal parameters are also obtained, which makes the method a suitable alternative for calibrating camera using a single image. Results from the two variants of the proposed approach are compared with Zhang's camera calibration algorithm and has been found to be better. Another utility of the proposed algorithm is to measure six dimensional pose of an industrial robot. Robot repeatability was also measured using the proposed camera calibration algorithm. The repeatability results were compared with the measurements using Artificial Reality toolkit {ArUco} which is a de-facto standard in the pose measurements using camera. The performance of the proposed method is better than that obtained from {ArUco}. Another area of application is identification of kinematic parameters. Using the circle point analysis method, identification of {KUKA} {KR}5 Arc robot was done using the proposed method.},
	eventtitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {2435--2440},
	booktitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Boby, R. A. and Saha, S. K.},
	urldate = {2021-02-24},
	date = {2016-05},
	langid = {english},
	file = {Boby and Saha - 2016 - Single image based camera calibration and pose est.pdf:C\:\\Users\\mosta\\Zotero\\storage\\FC65DHIR\\Boby and Saha - 2016 - Single image based camera calibration and pose est.pdf:application/pdf},
}

@inproceedings{briot_situ_2014,
	location = {Besacon},
	title = {In situ calibration of joint torque sensors of the {KUKA} {LightWeight} robot using only internal controller data},
	isbn = {978-1-4799-5736-1 978-1-4799-5735-4},
	url = {http://ieeexplore.ieee.org/document/6878122/},
	doi = {10.1109/AIM.2014.6878122},
	abstract = {The Kuka {LWR} is equipped with torque sensors mounted into the actuated joints. Each torque sensor is calibrated separately before it is mounted on the robot. This needs a second calibration at the last stage of the assembling of the robot in order to take into account the effect of the robot structure through it's jacobian matrix. This final calibration is necessary to improve the accuracy of the estimation of the interaction wrench of the robot with its environment. However, the proposed calibration techniques are usually complicated, time-consuming, and must be carried out before assembling the sensors on the robot. In this paper, a simple and fast method for calibrating the sensors once they are assembled on the robot is presented. The method is based on the least squares solution of an over-determined linear system obtained with the robot inverse dynamic identification model in which are included the sensor gains. This model is calculated with available sensor measurement and joint position sampled data while the robot is tracking some reference trajectories without load on the robot and some trajectories with a known payload fixed on the robot. The method is experimentally validated on the Kuka {LWR}4+ but can be applied to any similar kind of robot equipped with joint torque sensors.},
	eventtitle = {2014 {IEEE}/{ASME} International Conference on Advanced Intelligent Mechatronics ({AIM})},
	pages = {470--475},
	booktitle = {2014 {IEEE}/{ASME} International Conference on Advanced Intelligent Mechatronics},
	publisher = {{IEEE}},
	author = {Briot, S. and Gautier, M. and Jubien, A.},
	urldate = {2021-02-24},
	date = {2014-07},
	langid = {english},
	file = {Briot et al. - 2014 - In situ calibration of joint torque sensors of the.pdf:C\:\\Users\\mosta\\Zotero\\storage\\RS2XV4L2\\Briot et al. - 2014 - In situ calibration of joint torque sensors of the.pdf:application/pdf},
}